{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "This notebook will answer all the questions in part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\glend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\glend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\glend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from utils.rnn_model import *\n",
    "from utils.rnn_utils import *\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation \n",
    "\n",
    "First, we define all relevant hyperparameters, then we load the dataset.\n",
    "\n",
    "After which, we will load the word embeddings and process the data accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "trn_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "tst_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "\n",
    "We load the pre-trained Google News 300 dimension Word2Vec model and obtain a word index to be used later in data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====---------------------------------------------] 10.7% 178.7/1662.8MB downloaded"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word2vec_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_word2vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\glend\\Downloads\\SC4002 Project Github\\sc4002\\utils\\rnn_utils.py:59\u001b[0m, in \u001b[0;36mload_word2vec\u001b[1;34m(filepath, word2vec_api)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gensim\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mKeyedVectors\u001b[38;5;241m.\u001b[39mload(filepath)\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mword2vec-google-news-300\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\glend\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\downloader.py:496\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    494\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_dir, file_name)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(folder_dir):\n\u001b[1;32m--> 496\u001b[0m     \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_path:\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "File \u001b[1;32mc:\\Users\\glend\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\downloader.py:396\u001b[0m, in \u001b[0;36m_download\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    394\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{fname}\u001b[39;00m\u001b[38;5;124m.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(fname\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    395\u001b[0m dst_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmp_dir, fname)\n\u001b[1;32m--> 396\u001b[0m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreporthook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _calculate_md5_checksum(dst_path) \u001b[38;5;241m==\u001b[39m _get_checksum(name):\n\u001b[0;32m    398\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\glend\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\urllib\\request.py:277\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    275\u001b[0m             blocknum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    276\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m reporthook:\n\u001b[1;32m--> 277\u001b[0m                 \u001b[43mreporthook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocknum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m read \u001b[38;5;241m<\u001b[39m size:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentTooShortError(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieval incomplete: got only \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m out of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;241m%\u001b[39m (read, size), result)\n",
      "File \u001b[1;32mc:\\Users\\glend\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\downloader.py:123\u001b[0m, in \u001b[0;36m_progress\u001b[1;34m(chunks_downloaded, chunk_size, total_size, part, total_parts)\u001b[0m\n\u001b[0;32m    121\u001b[0m bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m filled_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m (bar_len \u001b[38;5;241m-\u001b[39m filled_len)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_parts \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 123\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m[\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m] \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43mMB downloaded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercent_downloaded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize_downloaded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtotal_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\iostream.py:694\u001b[0m, in \u001b[0;36mOutStream.write\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 694\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(string)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\iostream.py:590\u001b[0m, in \u001b[0;36mOutStream._schedule_flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_schedule_in_thread\u001b[39m():\n\u001b[0;32m    588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_io_loop\u001b[38;5;241m.\u001b[39mcall_later(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_interval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[1;32m--> 590\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_schedule_in_thread\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\iostream.py:267\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     f()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\zmq\\sugar\\socket.py:701\u001b[0m, in \u001b[0;36mSocket.send\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[0;32m    695\u001b[0m             data,\n\u001b[0;32m    696\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[0;32m    697\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    698\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[0;32m    699\u001b[0m         )\n\u001b[0;32m    700\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m--> 701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m_zmq.py:1092\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_zmq.py:1140\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_zmq.py:1339\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._send_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_zmq.py:160\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word2vec_model = load_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {\n",
    "    word: i for i, word in enumerate(\n",
    "        word2vec_model.index_to_key\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "\n",
    "Once we have the dataset and the word index both loaded, we can proceed with building the dataloaders for batch training. We first prepare the data by tokenizing and padding the data so that they are all of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_sentences, trn_labels = prepare_data(\n",
    "    trn_dataset[\"text\"],\n",
    "    trn_dataset[\"label\"],\n",
    "    word_index=word_index\n",
    ")\n",
    "val_sentences, val_labels = prepare_data(\n",
    "    val_dataset[\"text\"],\n",
    "    val_dataset[\"label\"],\n",
    "    word_index=word_index\n",
    ")\n",
    "tst_sentences, tst_labels = prepare_data(\n",
    "    tst_dataset[\"text\"],\n",
    "    tst_dataset[\"label\"],\n",
    "    word_index=word_index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is processed, we create dataloaders for the data for batch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dataloader = create_dataloader(\n",
    "    trn_sentences,\n",
    "    trn_labels,\n",
    "    BATCH_SIZE,\n",
    "    shuffle=True)\n",
    "val_dataloader = create_dataloader(\n",
    "    val_sentences,\n",
    "    val_labels,\n",
    "    BATCH_SIZE,\n",
    "    shuffle=False)\n",
    "tst_dataloader = create_dataloader(\n",
    "    tst_sentences,\n",
    "    tst_labels,\n",
    "    BATCH_SIZE,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we define all relevant hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "MODEL_TYPE = \"rnn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Default RNN\n",
    "\n",
    "We initialise the model for the default RNN without any extra processing to derive the final sentence representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(\n",
    "    embedding_dim=300,\n",
    "    hidden_size=256,\n",
    "    embedding_matrix=word2vec_model.vectors,\n",
    "    rnn_type=MODEL_TYPE,\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the data is loaded and processed into Dataloaders, we can start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100, Loss: 0.6527, Accuracy: 0.7214\n",
      "Model saved.\n",
      "Epoch   2/100, Loss: 0.5389, Accuracy: 0.7608\n",
      "Model saved.\n",
      "Epoch   3/100, Loss: 0.5156, Accuracy: 0.7486\n",
      "Epoch   4/100, Loss: 0.5026, Accuracy: 0.7561\n",
      "Epoch   5/100, Loss: 0.4955, Accuracy: 0.7477\n",
      "Epoch   6/100, Loss: 0.4916, Accuracy: 0.7467\n",
      "Epoch   7/100, Loss: 0.4894, Accuracy: 0.7439\n",
      "Epoch   8/100, Loss: 0.4778, Accuracy: 0.7636\n",
      "Model saved.\n",
      "Epoch   9/100, Loss: 0.4680, Accuracy: 0.7486\n",
      "Epoch  10/100, Loss: 0.4575, Accuracy: 0.7552\n",
      "Epoch  11/100, Loss: 0.4453, Accuracy: 0.7477\n",
      "Epoch  12/100, Loss: 0.4325, Accuracy: 0.7364\n",
      "Epoch  13/100, Loss: 0.4207, Accuracy: 0.7280\n",
      "Epoch  14/100, Loss: 0.4049, Accuracy: 0.7205\n",
      "Epoch  15/100, Loss: 0.3997, Accuracy: 0.7129\n",
      "Epoch  16/100, Loss: 0.3844, Accuracy: 0.7261\n",
      "Epoch  17/100, Loss: 0.3727, Accuracy: 0.7186\n",
      "Epoch  18/100, Loss: 0.3666, Accuracy: 0.7054\n",
      "Early stopping triggered after 18 epochs.\n",
      "Training ended, loading best model...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    trn_dataloader=trn_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    version=\"1\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_save_path=os.getenv(\"MODEL_SAVE_PATH\", \"modelfiles/\"),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=LR),\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10,\n",
    "    train_mode=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_and_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_and_plot\u001b[49m(\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      3\u001b[0m     trn_dataloader\u001b[38;5;241m=\u001b[39mtrn_dataloader,\n\u001b[0;32m      4\u001b[0m     val_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader,\n\u001b[0;32m      5\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m     model_type\u001b[38;5;241m=\u001b[39mMODEL_TYPE,\n\u001b[0;32m      7\u001b[0m     model_save_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODEL_SAVE_PATH\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelfiles/\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m      8\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLR),\n\u001b[0;32m      9\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[0;32m     10\u001b[0m     early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     11\u001b[0m     train_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_and_plot' is not defined"
     ]
    }
   ],
   "source": [
    "train_and_plot(\n",
    "    model=model,\n",
    "    trn_dataloader=trn_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    version=\"1\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_save_path=os.getenv(\"MODEL_SAVE_PATH\", \"modelfiles/\"),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=LR),\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10,\n",
    "    train_mode=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the validation check again to make sure we've loaded the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7636\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on the test set to obtain the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7364\n"
     ]
    }
   ],
   "source": [
    "tst_accuracy = validate(model, tst_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Last State RNN\n",
    "\n",
    "This RNN will pick the hidden vector from the last time step as the sentence representation. This approach assumes that the last hidden state will capture the overall meaning of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(\n",
    "    embedding_dim=300,\n",
    "    hidden_size=256,\n",
    "    embedding_matrix=word2vec_model.vectors,\n",
    "    rnn_type=MODEL_TYPE,\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the data is loaded and processed into Dataloaders, we can start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100, Loss: 0.6648, Accuracy: 0.7392\n",
      "Model saved.\n",
      "Epoch   2/100, Loss: 0.5354, Accuracy: 0.7486\n",
      "Model saved.\n",
      "Epoch   3/100, Loss: 0.5125, Accuracy: 0.7533\n",
      "Model saved.\n",
      "Epoch   4/100, Loss: 0.5042, Accuracy: 0.7495\n",
      "Epoch   5/100, Loss: 0.4986, Accuracy: 0.7552\n",
      "Model saved.\n",
      "Epoch   6/100, Loss: 0.4895, Accuracy: 0.7514\n",
      "Epoch   7/100, Loss: 0.4850, Accuracy: 0.7505\n",
      "Epoch   8/100, Loss: 0.4801, Accuracy: 0.7542\n",
      "Epoch   9/100, Loss: 0.4712, Accuracy: 0.7505\n",
      "Epoch  10/100, Loss: 0.4624, Accuracy: 0.7439\n",
      "Epoch  11/100, Loss: 0.4498, Accuracy: 0.7439\n",
      "Epoch  12/100, Loss: 0.4383, Accuracy: 0.7383\n",
      "Epoch  13/100, Loss: 0.4258, Accuracy: 0.7317\n",
      "Epoch  14/100, Loss: 0.4160, Accuracy: 0.7270\n",
      "Epoch  15/100, Loss: 0.4035, Accuracy: 0.7336\n",
      "Early stopping triggered after 15 epochs.\n",
      "Training ended, loading best model...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    trn_dataloader=trn_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    version=\"1\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_save_path=os.getenv(\"MODEL_SAVE_PATH\", \"modelfiles/\"),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=LR),\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10,\n",
    "    train_mode=\"last_state\" # train only the last state\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot(\n",
    "    model=model,\n",
    "    trn_dataloader=trn_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    version=\"1\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_save_path=os.getenv(\"MODEL_SAVE_PATH\", \"modelfiles/\"),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=LR),\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10,\n",
    "    train_mode=\"last_state\" # train only the last state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the validation check again to make sure we've loaded the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7552\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on the test set to obtain the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7373\n"
     ]
    }
   ],
   "source": [
    "tst_accuracy = validate(model, tst_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Mean Pooling RNN\n",
    "\n",
    "This RNN will use the average of all hidden vectors as the sentence representation. This captures information across the whole sentence by averaging the all the words' contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(\n",
    "    embedding_dim=300,\n",
    "    hidden_size=256,\n",
    "    embedding_matrix=word2vec_model.vectors,\n",
    "    rnn_type=MODEL_TYPE,\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the data is loaded and processed into Dataloaders, we can start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100, Loss: 0.6644, Accuracy: 0.7064\n",
      "Model saved.\n",
      "Epoch   2/100, Loss: 0.5367, Accuracy: 0.7111\n",
      "Model saved.\n",
      "Epoch   3/100, Loss: 0.5131, Accuracy: 0.7505\n",
      "Model saved.\n",
      "Epoch   4/100, Loss: 0.5049, Accuracy: 0.7542\n",
      "Model saved.\n",
      "Epoch   5/100, Loss: 0.4977, Accuracy: 0.7411\n",
      "Epoch   6/100, Loss: 0.4920, Accuracy: 0.7533\n",
      "Epoch   7/100, Loss: 0.4860, Accuracy: 0.7533\n",
      "Epoch   8/100, Loss: 0.4812, Accuracy: 0.7552\n",
      "Model saved.\n",
      "Epoch   9/100, Loss: 0.4692, Accuracy: 0.7533\n",
      "Epoch  10/100, Loss: 0.4637, Accuracy: 0.7261\n",
      "Epoch  11/100, Loss: 0.4526, Accuracy: 0.7477\n",
      "Epoch  12/100, Loss: 0.4387, Accuracy: 0.7411\n",
      "Epoch  13/100, Loss: 0.4278, Accuracy: 0.7364\n",
      "Epoch  14/100, Loss: 0.4188, Accuracy: 0.7326\n",
      "Epoch  15/100, Loss: 0.4084, Accuracy: 0.7345\n",
      "Epoch  16/100, Loss: 0.3991, Accuracy: 0.7139\n",
      "Epoch  17/100, Loss: 0.3915, Accuracy: 0.7289\n",
      "Epoch  18/100, Loss: 0.3753, Accuracy: 0.7073\n",
      "Early stopping triggered after 18 epochs.\n",
      "Training ended, loading best model...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    trn_dataloader=trn_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    version=\"1\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_save_path=os.getenv(\"MODEL_SAVE_PATH\", \"modelfiles/\"),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=LR),\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10,\n",
    "    train_mode=\"mean_pool\" # train only the mean pool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot(\n",
    "    model=model,\n",
    "    trn_dataloader=trn_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    version=\"1\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_save_path=os.getenv(\"MODEL_SAVE_PATH\", \"modelfiles/\"),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=LR),\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10,\n",
    "    train_mode=\"mean_pool\" # train only the mean pool\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the validation check again to make sure we've loaded the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7552\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on the test set to obtain the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7402\n"
     ]
    }
   ],
   "source": [
    "tst_accuracy = validate(model, tst_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Max Pool RNN\n",
    "\n",
    "This RNN will compute the max of all hidden vectors along each dimension. This will effectively use the most significant word as the representation of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(\n",
    "    embedding_dim=300,\n",
    "    hidden_size=256,\n",
    "    embedding_matrix=word2vec_model.vectors,\n",
    "    rnn_type=MODEL_TYPE,\n",
    "    bidirectional=False,\n",
    "    num_layers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the data is loaded and processed into Dataloaders, we can start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100, Loss: 0.6549, Accuracy: 0.7402\n",
      "Model saved.\n",
      "Epoch   2/100, Loss: 0.5338, Accuracy: 0.7477\n",
      "Model saved.\n",
      "Epoch   3/100, Loss: 0.5127, Accuracy: 0.7561\n",
      "Model saved.\n",
      "Epoch   4/100, Loss: 0.5037, Accuracy: 0.7439\n",
      "Epoch   5/100, Loss: 0.4981, Accuracy: 0.7561\n",
      "Epoch   6/100, Loss: 0.4927, Accuracy: 0.7505\n",
      "Epoch   7/100, Loss: 0.4880, Accuracy: 0.7523\n",
      "Epoch   8/100, Loss: 0.4827, Accuracy: 0.7636\n",
      "Model saved.\n",
      "Epoch   9/100, Loss: 0.4767, Accuracy: 0.7664\n",
      "Model saved.\n",
      "Epoch  10/100, Loss: 0.4693, Accuracy: 0.7439\n",
      "Epoch  11/100, Loss: 0.4608, Accuracy: 0.7495\n",
      "Epoch  12/100, Loss: 0.4546, Accuracy: 0.7495\n",
      "Epoch  13/100, Loss: 0.4397, Accuracy: 0.7308\n",
      "Epoch  14/100, Loss: 0.4280, Accuracy: 0.7345\n",
      "Epoch  15/100, Loss: 0.4196, Accuracy: 0.7326\n",
      "Epoch  16/100, Loss: 0.4094, Accuracy: 0.7036\n",
      "Epoch  17/100, Loss: 0.3970, Accuracy: 0.7345\n",
      "Epoch  18/100, Loss: 0.3856, Accuracy: 0.7289\n",
      "Epoch  19/100, Loss: 0.3728, Accuracy: 0.7120\n",
      "Early stopping triggered after 19 epochs.\n",
      "Training ended, loading best model...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    trn_dataloader=trn_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    version=\"1\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_save_path=os.getenv(\"MODEL_SAVE_PATH\", \"modelfiles/\"),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=LR),\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10,\n",
    "    train_mode=\"max_pool\" # train only the max pool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot(\n",
    "    model=model,\n",
    "    trn_dataloader=trn_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    version=\"1\",\n",
    "    model_type=MODEL_TYPE,\n",
    "    model_save_path=os.getenv(\"MODEL_SAVE_PATH\", \"modelfiles/\"),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=LR),\n",
    "    epochs=100,\n",
    "    early_stopping_patience=10,\n",
    "    train_mode=\"max_pool\" # train only the max pool\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the validation check again to make sure we've loaded the right model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7664\n"
     ]
    }
   ],
   "source": [
    "val_accuracy = validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on the test set to obtain the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7430\n"
     ]
    }
   ],
   "source": [
    "tst_accuracy = validate(model, tst_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the 4 visualisations, we can see that the best performing model is..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
